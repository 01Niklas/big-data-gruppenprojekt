{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.122974Z",
     "start_time": "2025-06-07T14:42:33.117598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "from abc import abstractmethod\n",
    "from datetime import datetime\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n"
   ],
   "id": "e6fd62392feb88db",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.142959Z",
     "start_time": "2025-06-07T14:42:33.136883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Recommender:\n",
    "    def __init__(self):\n",
    "        self.k = 3 # default\n",
    "        self.user_id = None\n",
    "        self.item_id = None\n",
    "        self.similarity: Literal[\"cosine\", \"pearson\"] = \"cosine\"  # default\n",
    "        self.calculation_variant: Literal[\"weighted\", \"unweighted\"] = \"weighted\"  # default\n",
    "        self.data = None\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "    def _prepare_information(self, user_id: str, item_id: str, k: int, similarity: Literal[\"cosine\", \"pearson\"] = \"cosine\", calculation_variant: Literal[\"weighted\", \"unweighted\"] = \"weighted\") -> None:\n",
    "        self.user_id = user_id\n",
    "        self.item_id = item_id\n",
    "        self.similarity = similarity\n",
    "        self.calculation_variant = calculation_variant\n",
    "        self.k = k\n",
    "\n",
    "        if similarity == 'pearson' and self.data is not None:\n",
    "            self.data['mean'] = self.data.mean(axis=1)\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(\n",
    "            self,\n",
    "            user_id: str,\n",
    "            item_id: str,\n",
    "            similarity: Optional[Literal['cosine', 'pearson']] = 'cosine',   # only for collaborative filtering\n",
    "            calculation_variety: Optional[Literal['weighted', 'unweighted']] = 'weighted',  # only for collaborative filtering\n",
    "            k: Optional[int] = 3,\n",
    "            second_k_value: Optional[int] = None):\n",
    "        ..."
   ],
   "id": "813ebe70194fc0a4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.191456Z",
     "start_time": "2025-06-07T14:42:33.173523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CollaborativeFilteringRecommender(Recommender):\n",
    "    def __init__(self, data: pd.DataFrame, mode: Literal['user', 'item'] = 'user', display_results_for_each_step: Optional[bool] = False) -> None:\n",
    "        super().__init__()\n",
    "        self.display_results_for_each_step = display_results_for_each_step\n",
    "        self.original_data = data\n",
    "        self.mode = mode\n",
    "        self._preprocess_data()\n",
    "\n",
    "\n",
    "    def _preprocess_data(self) -> None:\n",
    "        self.original_data = self.original_data.set_index(\"user_ID\")\n",
    "        self.original_data.index = self.original_data.index.astype(str) # convert the index to string (due to error with int values)\n",
    "        if self.mode == 'item':\n",
    "            self.data = self.original_data.T  # transpose for item based\n",
    "        else:\n",
    "            self.data = self.original_data  # original for user based\n",
    "\n",
    "\n",
    "    def _calculate_distance_and_indices(self, dataframe: pd.DataFrame) -> ([], []):\n",
    "        knn = NearestNeighbors(metric=\"cosine\", algorithm='brute')\n",
    "        knn.fit(dataframe.values)\n",
    "        distances, indices = knn.kneighbors(dataframe.values, n_neighbors=self.k + 1)\n",
    "\n",
    "        if self.mode == 'item':\n",
    "            index = dataframe.index.get_loc(self.item_id)\n",
    "        else:\n",
    "            index = dataframe.index.get_loc(self.user_id)\n",
    "\n",
    "        similar_distances = distances[index, 1:]\n",
    "        similar_indices = indices[index, 1:]\n",
    "\n",
    "        return similar_distances, similar_indices\n",
    "\n",
    "    def _calculate_similarities(self, similar_distances: np.ndarray) -> np.ndarray:\n",
    "        similarity = [1 - x for x in similar_distances]\n",
    "        similarity = [(y + 1) / 2 for y in similarity]\n",
    "        return np.array(similarity)\n",
    "\n",
    "    def _calculate_result(self, similarity: np.ndarray, ratings: np.ndarray) -> float:\n",
    "        if self.calculation_variant == \"weighted\":\n",
    "            mean = np.dot(ratings, similarity) / similarity.sum()\n",
    "            return mean\n",
    "        else:\n",
    "            return float(np.mean(ratings))\n",
    "\n",
    "    def _check_values(self) -> None:\n",
    "        if self.mode == 'user':\n",
    "            if self.user_id not in self.data.index:\n",
    "                raise ValueError(f\"User {self.user_id} nicht in Daten.\")\n",
    "            if self.item_id not in self.data.columns:\n",
    "                raise ValueError(f\"Item {self.item_id} nicht in Daten.\")\n",
    "        elif self.mode == 'item':\n",
    "            if self.user_id not in self.original_data.index:\n",
    "                raise ValueError(\n",
    "                    f\"User {self.user_id} nicht in Originaldaten.\")\n",
    "            if self.item_id not in self.data.index:\n",
    "                raise ValueError(\n",
    "                    f\"Item {self.item_id} nicht in transponierten Daten.\")\n",
    "\n",
    "    def _process_item_based(self) -> pd.DataFrame:\n",
    "        user_ratings = self.original_data.loc[self.user_id]\n",
    "\n",
    "        # filter based on the item. Only the users that already gave a rating are relevant\n",
    "        rated_items = user_ratings[user_ratings > 0.0].index.tolist()\n",
    "\n",
    "        if not rated_items:\n",
    "            raise ValueError(f\"User {self.user_id} hat keine Items bewertet!\")\n",
    "\n",
    "        return self.data.loc[rated_items + [self.item_id]]\n",
    "\n",
    "    def _process_user_based(self) -> pd.DataFrame:\n",
    "        # filter based on the item. Only the users that already gave a rating are relevant\n",
    "        relevant_df = self.data[self.data[self.item_id] > 0.0]\n",
    "\n",
    "        # add the user we are looking for (due to non-existing rating this user where filtered out)\n",
    "        return pd.concat([relevant_df, self.data.loc[[self.user_id]]])\n",
    "\n",
    "    def _normalize_for_pearson(self, relevant_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        mean_values = relevant_df.mean(axis=1).to_numpy()\n",
    "        relevant_df = relevant_df.sub(mean_values, axis=0)\n",
    "        return relevant_df\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            user_id: str,\n",
    "            item_id: str,\n",
    "            similarity: Literal['cosine', 'pearson'] = 'cosine',\n",
    "            calculation_variety: Literal['weighted', 'unweighted'] = 'weighted',\n",
    "            k: Optional[int] = 3,\n",
    "            second_k_value: Optional[int] = None) -> float:\n",
    "        self._prepare_information(user_id=user_id, item_id=item_id, similarity=similarity, calculation_variant=calculation_variety, k=k)\n",
    "        self._check_values()\n",
    "\n",
    "        if self.mode == 'item':\n",
    "            relevant_df = self._process_item_based()\n",
    "        else:\n",
    "            relevant_df = self._process_user_based()\n",
    "\n",
    "        if similarity == 'pearson':\n",
    "            self._normalize_for_pearson(relevant_df)\n",
    "\n",
    "        # make sure that there are no NaN values -> set NaN to 0.0\n",
    "        relevant_df = relevant_df.fillna(0.0)\n",
    "        similar_distances, similar_indices = self._calculate_distance_and_indices(dataframe=relevant_df)\n",
    "\n",
    "        if self.mode == 'item':\n",
    "            ratings = relevant_df.iloc[similar_indices][self.user_id].to_numpy()\n",
    "        else:\n",
    "            ratings = relevant_df.iloc[similar_indices][self.item_id].to_numpy()\n",
    "\n",
    "        similarity = self._calculate_similarities(similar_distances)\n",
    "        result = self._calculate_result(similarity, ratings)\n",
    "\n",
    "        if self.display_results_for_each_step:\n",
    "          self.explain(similar_indices, relevant_df, ratings, similarity, result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def explain(self, similar_indices, relevant_df, ratings, similarity, result) -> None:\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"<mode: {self.mode}>\")\n",
    "        print(f\"({self.calculation_variant}) Mittelwert: {result:.4f}\")\n",
    "        print(f\"Metrik: {self.similarity}\")\n",
    "        print()\n",
    "        print(f\"k ({self.k}) Ã¤hnlichsten {'Items' if self.mode == 'item' else 'Nutzer'}:\")\n",
    "        df = pd.DataFrame({\n",
    "            \"ID\": relevant_df.index[similar_indices],\n",
    "            \"rating\": ratings,\n",
    "            \"similarity\": similarity\n",
    "        }).reset_index(drop=True)\n",
    "        print(df.to_string(index=True, header=True))\n",
    "        print(\"-\" * 50)\n"
   ],
   "id": "cad50448f6df9a05",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.241609Z",
     "start_time": "2025-06-07T14:42:33.222675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ContentBasedRecommender(Recommender):\n",
    "    def __init__(self, item_profile: pd.DataFrame, user_ratings: pd.DataFrame) -> None:\n",
    "        super().__init__()\n",
    "        self.item_profile = item_profile\n",
    "        self.user_ratings = user_ratings\n",
    "        self.k = 3\n",
    "        self.feature_matrix = None\n",
    "        self._preprocess_data()\n",
    "\n",
    "        # check if the features \"budget\", \"revenue\", \"runtime\" are relevant for the item/rating correlation\n",
    "        self._check_features_correlation(features=[\"budget\", \"revenue\", \"runtime\"])\n",
    "        self._calculate_tfidf_matrix()\n",
    "\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        self.item_profile[\"item_ID\"] = self.item_profile[\"item_ID\"].astype(str)\n",
    "        self.user_ratings[\"item_ID\"] = self.user_ratings[\"item_ID\"].astype(str)\n",
    "        self.user_ratings[\"user_ID\"] = self.user_ratings[\"user_ID\"].astype(str)\n",
    "\n",
    "    def _check_features_correlation(self, features: List[str]) -> None:\n",
    "        irrelevant_features = []  #  list for irrelevant feature that will be removed\n",
    "\n",
    "        for feature in features:\n",
    "            if feature not in self.item_profile.columns:\n",
    "                continue\n",
    "\n",
    "            # combine item and user ratings\n",
    "            merged_data = pd.merge(self.user_ratings, self.item_profile, on=\"item_ID\")\n",
    "\n",
    "            # convert to numeric\n",
    "            feature_data = pd.to_numeric(merged_data[feature].fillna(0), errors=\"coerce\")\n",
    "            rating_data = pd.to_numeric(merged_data[\"rating\"].fillna(0), errors=\"coerce\")\n",
    "\n",
    "            # calculate the correlation between the user rating and the feature\n",
    "            correlation, p_value = stats.pearsonr(feature_data, rating_data)\n",
    "\n",
    "            # check if the correlation is relevant / significant\n",
    "            if abs(correlation) < 0.1 or p_value > 0.05:\n",
    "                logger.debug(f\"Feature '{feature}' does not have a sigificant correlation and will be ignored.\")\n",
    "                irrelevant_features.append(feature)\n",
    "            else:\n",
    "                logger.debug(f\"Feature '{feature}' has a significant correlation: {correlation}\")\n",
    "\n",
    "        self.item_profile.drop(columns=irrelevant_features, inplace=True)\n",
    "\n",
    "\n",
    "    def _safe_get_feature(self, feature_name):\n",
    "        if feature_name in self.item_profile.columns:\n",
    "            return self.item_profile[feature_name]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _calculate_tfidf_matrix(self) -> None:\n",
    "        # optional but if the title is empty we set it as an empty string\n",
    "        self.item_profile[\"title\"] = self.item_profile[\"title\"].fillna(\"\")\n",
    "\n",
    "        # use the TfidfVectorizer() to transform title into numerical feature\n",
    "        title_vectorizer = TfidfVectorizer()\n",
    "        title_features = title_vectorizer.fit_transform(self.item_profile[\"title\"])\n",
    "\n",
    "        # change genre columns in text by just extracting the word after '\"Genre_\"'\n",
    "        genre_cols = [col for col in self.item_profile.columns if col.startswith(\"Genre_\")]\n",
    "        if genre_cols:\n",
    "            self.item_profile[\"genre_text\"] = self.item_profile[genre_cols].astype(int).apply(\n",
    "                lambda row: \" \".join([col.replace(\"Genre_\", \"\") for col, val in row.items() if val == 1]), axis=1\n",
    "            )\n",
    "            genre_vectorizer = TfidfVectorizer()\n",
    "            genre_features = genre_vectorizer.fit_transform(self.item_profile[\"genre_text\"])\n",
    "        else:\n",
    "            genre_features = np.empty((len(self.item_profile), 0))\n",
    "\n",
    "        # the language of the items transformed into one-hot-encoded-dummies\n",
    "        language_dummies = pd.get_dummies(self.item_profile[\"original_language\"], prefix=\"lang\")\n",
    "\n",
    "        # put runtime into three categories (short, medium, long)\n",
    "        runtime_feature = self._safe_get_feature(\"runtime\")\n",
    "        if runtime_feature is not None:\n",
    "            runtime_bucket = pd.qcut(runtime_feature, q=3, labels=[\"kurz\", \"mittel\", \"lang\"])\n",
    "            runtime_dummies = pd.get_dummies(runtime_bucket, prefix=\"runtime\")\n",
    "        else:\n",
    "            runtime_dummies = pd.DataFrame(index=self.item_profile.index)\n",
    "\n",
    "        # budget and include will be logarithmically transformed and then scaled\n",
    "        numerical_features = []\n",
    "        if \"budget\" in self.item_profile.columns:\n",
    "            self.item_profile[\"log_budget\"] = np.log1p(self.item_profile[\"budget\"].fillna(0))\n",
    "            numerical_features.append(\"log_budget\")\n",
    "        if \"revenue\" in self.item_profile.columns:\n",
    "            self.item_profile[\"log_revenue\"] = np.log1p(self.item_profile[\"revenue\"].fillna(0))\n",
    "            numerical_features.append(\"log_revenue\")\n",
    "\n",
    "        if numerical_features:\n",
    "            scaler = StandardScaler()\n",
    "            scaled_numericals = scaler.fit_transform(self.item_profile[numerical_features])\n",
    "        else:\n",
    "            scaled_numericals = np.empty((len(self.item_profile), 0))\n",
    "\n",
    "        # create feature matrix\n",
    "        self.feature_matrix = hstack([\n",
    "            title_features,\n",
    "            genre_features,\n",
    "            language_dummies.values,\n",
    "            runtime_dummies.values,\n",
    "            scaled_numericals\n",
    "        ])\n",
    "\n",
    "        self.feature_matrix = csr_matrix(self.feature_matrix)\n",
    "\n",
    "    def _check_values(self):\n",
    "        if self.user_id not in self.user_ratings[\"user_ID\"].values:\n",
    "            raise ValueError(f\"User-ID {self.user_id} not found.\")\n",
    "\n",
    "        if self.item_id not in self.item_profile[\"item_ID\"].values:\n",
    "            raise ValueError(f\"Item-ID {self.item_id} not found.\")\n",
    "\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            user_id: str,\n",
    "            item_id: str,\n",
    "            similarity: Optional[Literal['cosine', 'pearson']] = 'cosine',  # only for collaborative filtering\n",
    "            calculation_variety: Optional[Literal['weighted', 'unweighted']] = 'weighted', # only for collaborative filtering\n",
    "            k: Optional[int] = 3,\n",
    "            second_k_value: Optional[int] = None) -> float:\n",
    "\n",
    "        # default function to save all the information\n",
    "        self._prepare_information(user_id=user_id, item_id=item_id, k=k)\n",
    "\n",
    "        # check if the values included in the dataframes\n",
    "        self._check_values()\n",
    "\n",
    "        # extract only the items, the user rated\n",
    "        rated_items = self.user_ratings[self.user_ratings[\"user_ID\"] == user_id]\n",
    "        rated_item_ids = rated_items[\"item_ID\"].values\n",
    "\n",
    "        # this case can happen when k is greater than the rated items by the user\n",
    "        if self.k > len(rated_item_ids):\n",
    "            self.k = len(rated_item_ids)\n",
    "\n",
    "        # extract the rated item indices from the item profile\n",
    "        rated_item_indices = self.item_profile[self.item_profile[\"item_ID\"].isin(rated_item_ids)].index\n",
    "\n",
    "        # check if the user rated some items... if not then return 0.0\n",
    "        if len(rated_item_indices) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # get the feature matrix that is calculated in the '_calculate_tfidf_matrix()'-Method\n",
    "        filtered_matrix = self.feature_matrix[rated_item_indices]\n",
    "\n",
    "        # default kNN usage like in the lecture with brute algorithm and cosine as metric\n",
    "        knn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "        knn.fit(filtered_matrix)\n",
    "\n",
    "        item_index = self.item_profile[self.item_profile[\"item_ID\"] == item_id].index[0]\n",
    "        distances, indices = knn.kneighbors(self.feature_matrix[item_index], n_neighbors=self.k + 1)  # k+1 because the item itself is also included\n",
    "\n",
    "        # the similar item indices beginning with the first real neighbor\n",
    "        similar_items = indices.flatten()[1:]\n",
    "        similar_item_indices = rated_item_indices[similar_items]\n",
    "\n",
    "        # extract for each item in the similar item indices list the rating and save it in the list\n",
    "        similar_ratings = []\n",
    "        for idx in similar_item_indices:\n",
    "            similar_item_id = self.item_profile.iloc[idx][\"item_ID\"]\n",
    "            user_rating = self.user_ratings[(self.user_ratings[\"user_ID\"] == user_id) & (self.user_ratings[\"item_ID\"] == similar_item_id)]\n",
    "            if not user_rating.empty:\n",
    "                similar_ratings.append(user_rating[\"rating\"].values[0])\n",
    "\n",
    "        # if the similar ratings is zero then we return a default 0.0\n",
    "        if not similar_ratings:\n",
    "            return 0.0\n",
    "\n",
    "        # calculate the predicted rating based on the sum of ratings and len of ratings\n",
    "        return sum(similar_ratings) / len(similar_ratings)"
   ],
   "id": "6928825fe8030222",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.263811Z",
     "start_time": "2025-06-07T14:42:33.257536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HybridRecommender(Recommender):\n",
    "    def __init__(self, data: pd.DataFrame, item_profile: pd.DataFrame, user_ratings: pd.DataFrame, mode: Literal['user', 'item'] = 'user', alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.collaborative_recommender = CollaborativeFilteringRecommender(data=data, mode=mode)\n",
    "        self.content_based_recommender = ContentBasedRecommender(item_profile=item_profile, user_ratings=user_ratings)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            user_id: str,\n",
    "            item_id: str,\n",
    "            similarity: Optional[Literal['cosine', 'pearson']] = 'cosine',  # only for collaborative filtering\n",
    "            calculation_variety: Optional[Literal['weighted', 'unweighted']] = 'weighted', # only for collaborative filtering\n",
    "            k: Optional[int] = 3,\n",
    "            second_k_value: Optional[int] = 3):\n",
    "\n",
    "        collaborative_prediction = self.collaborative_recommender.predict(\n",
    "            user_id=user_id,\n",
    "            item_id=item_id,\n",
    "            similarity=similarity, # ignore that it can be NONE\n",
    "            calculation_variety=calculation_variety, # ignore that it can be NONE\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        content_based_prediction = self.content_based_recommender.predict(\n",
    "            user_id=user_id,\n",
    "            item_id=item_id,\n",
    "            similarity=similarity,\n",
    "            calculation_variety=calculation_variety,\n",
    "            k=second_k_value\n",
    "        )\n",
    "\n",
    "        # combine both with alpha as weight\n",
    "        combined_prediction = (self.alpha * collaborative_prediction) + ((1 - self.alpha) * content_based_prediction)\n",
    "        return combined_prediction"
   ],
   "id": "c8fbff7bb11954ad",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.300352Z",
     "start_time": "2025-06-07T14:42:33.279539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Test(BaseModel):\n",
    "    name: str\n",
    "    type: Literal[\"collaborative_filtering\", \"content_based\", \"hybrid\"]\n",
    "    mode: Optional[Literal[\"user\", \"item\"]] = \"item\"\n",
    "    k_value: int\n",
    "    second_k_value: Optional[int] = 3\n",
    "    metric: Optional[Literal[\"cosine\", \"pearson\"]] = 'cosine'\n",
    "    calculation_variety: Optional[Literal[\"weighted\", \"unweighted\"]] = 'weighted'\n",
    "    alpha: Optional[float] = 0.5\n",
    "\n",
    "\n",
    "class TestResult(BaseModel):\n",
    "    name: str\n",
    "    type: Literal[\"collaborative_filtering\", \"content_based\", \"hybrid\"]\n",
    "    mode: Literal[\"user\", \"item\"]\n",
    "    k_value: int\n",
    "    metric: Literal[\"cosine\", \"pearson\"]\n",
    "    calculation_variety: Literal[\"weighted\", \"unweighted\"]\n",
    "    alpha: float\n",
    "    mae: float\n",
    "\n",
    "\n",
    "class TestResults(BaseModel): # just for saving in a \"pretty\" form\n",
    "    date: str\n",
    "    num_tests: int\n",
    "    best_test: TestResult\n",
    "    results: List[TestResult]\n",
    "\n",
    "\n",
    "\n",
    "class MAETester:\n",
    "    def __init__(self, tests: List[Test], test_data_path: str, data_path: str, item_profile_path: str, user_ratings: str):\n",
    "        self.tests = tests\n",
    "        self.testdata = pd.read_csv(test_data_path)  # testdata (for evaluaton)\n",
    "        self.item_profile = pd.read_csv(item_profile_path)\n",
    "        self.user_ratings = pd.read_csv(user_ratings)\n",
    "        self._prepare_data()\n",
    "        self.data = pd.read_csv(data_path)  # trainings-data\n",
    "        self.results: List[TestResult] = []\n",
    "\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        self.testdata[\"user_ID\"] = self.testdata[\"user_ID\"].astype(str)\n",
    "        self.testdata[\"item_ID\"] = self.testdata[\"item_ID\"].astype(str)\n",
    "\n",
    "\n",
    "    def run_tests(self) -> pd.DataFrame:\n",
    "        for test in self.tests:\n",
    "            result = self._run_test(test)\n",
    "            self.results.append(result)\n",
    "            logger.success(f\"Test abgeschlossen: {test.name}, MAE: {result.mae:.4f}\\n\")\n",
    "\n",
    "        # display final results\n",
    "        result_df = self._summarize_test_results()\n",
    "\n",
    "        # save final results to file\n",
    "        self._save_to_file()\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def _run_test(self, test: Test) -> TestResult:\n",
    "        logger.info(f\"Running test: {test.name}\")\n",
    "\n",
    "        if test.type == \"content_based\":\n",
    "            recommender = ContentBasedRecommender(\n",
    "                item_profile=self.item_profile,\n",
    "                user_ratings=self.user_ratings,\n",
    "            )\n",
    "        elif test.type == \"collaborative_filtering\":\n",
    "            recommender = CollaborativeFilteringRecommender(\n",
    "                mode=test.mode, # ignore type (that this can be NONE)\n",
    "                data=self.data,\n",
    "            )\n",
    "        elif test.type == \"hybrid\":\n",
    "            recommender = HybridRecommender(\n",
    "                data=self.data,\n",
    "                item_profile=self.item_profile,\n",
    "                user_ratings=self.user_ratings,\n",
    "                mode=test.mode,  # ignore type (that this can be NONE)\n",
    "                alpha=test.alpha,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unbekannter Recomendertyp: {test.type}\")\n",
    "\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "\n",
    "        testdata_list = self.testdata.to_numpy()\n",
    "\n",
    "        for row in tqdm(testdata_list, desc=\"Vorhersagen werden berechnet\"):\n",
    "            user_id: str = str(row[0])\n",
    "            item_id: str = str(row[1])\n",
    "            actual_rating = row[2]\n",
    "\n",
    "            try:\n",
    "                predicted_rating = recommender.predict(\n",
    "                    user_id=user_id,\n",
    "                    item_id=item_id,\n",
    "                    similarity=test.metric,\n",
    "                    calculation_variety=test.calculation_variety,\n",
    "                    k=test.k_value,\n",
    "                    second_k_value=test.second_k_value,\n",
    "                )\n",
    "                predictions.append(predicted_rating)\n",
    "                actuals.append(actual_rating)\n",
    "            except ValueError as e:\n",
    "                logger.warning(f\"Fehler bei der Vorhersage: {e}\")\n",
    "\n",
    "        mae = self._mean_absolute_error(actuals, predictions)\n",
    "\n",
    "        return TestResult(\n",
    "            name=test.name,\n",
    "            type=test.type,\n",
    "            mode=test.mode,\n",
    "            k_value=test.k_value,\n",
    "            metric=test.metric,\n",
    "            calculation_variety=test.calculation_variety,\n",
    "            alpha=test.alpha,\n",
    "            mae=mae,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_absolute_error(actuals: List[float], predictions: List[float]) -> float:\n",
    "        if not actuals or not predictions or len(actuals) != len(predictions):\n",
    "            raise ValueError(\"Listen fÃ¼r tatsÃ¤chliche und vorhergesagte Werte mÃ¼ssen gleich lang und nicht leer sein.\")\n",
    "\n",
    "        absolute_errors = [abs(a - p) for a, p in zip(actuals, predictions)]\n",
    "        mae = sum(absolute_errors) / len(absolute_errors)\n",
    "        return mae\n",
    "\n",
    "    def _summarize_test_results(self) -> pd.DataFrame:\n",
    "        if not self.results:\n",
    "            logger.info(\"Keine Testergebnisse vorhanden.\")\n",
    "            return\n",
    "\n",
    "        summary_df = pd.DataFrame([{\n",
    "            \"Testname\": result.name,\n",
    "            \"Recomendertyp\": result.type,\n",
    "            \"Modus\": result.mode if result.type == \"collaborative_filtering\" else \"/\",\n",
    "            \"k-Wert\": result.k_value,\n",
    "            \"Metrik\": result.metric if result.type == \"collaborative_filtering\" else \"/\",\n",
    "            \"Berechnungsvariante\": result.calculation_variety if result.type == \"collaborative_filtering\" else \"/\",\n",
    "            \"Alpha (weight)\" : result.alpha if result.type == \"hybrid\" else \"/\",\n",
    "            \"MAE\": result.mae\n",
    "        } for result in self.results])\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Zusammenfassung der Testergebnisse:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        return summary_df\n",
    "\n",
    "\n",
    "    def _save_to_file(self) -> None:\n",
    "        if not self.results:\n",
    "            logger.info(\"Keine Testergebnisse vorhanden, nichts zu speichern.\")\n",
    "            return\n",
    "        date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        best_test = min(self.results, key=lambda result: result.mae)  # best results based on mae\n",
    "        test_results = TestResults(\n",
    "            date=date,\n",
    "            num_tests=len(self.results),\n",
    "            best_test=best_test,\n",
    "            results=self.results\n",
    "        )\n",
    "\n",
    "        file_path = f\"./outputs/testergebnis_{date.replace(':', '-')}.json\"\n",
    "\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json_file.write(test_results.model_dump_json(indent=4))\n",
    "\n",
    "        logger.success(f\"Testergebnisse erfolgreich gespeichert.\")\n",
    "\n",
    "\n"
   ],
   "id": "bd623ba6223a9825",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Beginn der Aufrufe und Nutzung der Recommender",
   "id": "d924f474e12c71d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "wir haben viele TestfÃ¤lle durchgefÃ¼hrt (diese sind ganz unten zu finden) und uns dann aufgrund der MAE-Ergebnisse fÃ¼r folgenden Hybriden Recommender entschieden der sowohl CollaborativeFiltering als auch Conten-Based-Filtering berÃ¼cksichtigt.",
   "id": "6af79d55b715866d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Um diesen Recommender zu testen stellen wir eine struktur bereit wobei lediglich der parameter `testdata` angepasst werden muss um den evaluationsdatensatz zu verwenden. Aufrgund der sehr nah beieinanderliegenden Ergebnisse fÃ¼r pearson und cosine stellen wir beide Testprofile zur VerfÃ¼gung.",
   "id": "4593c5f25d4441af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.313989Z",
     "start_time": "2025-06-07T14:42:33.311203Z"
    }
   },
   "cell_type": "code",
   "source": "testdata_path = \"./data/Testdaten_FlixNet.csv\"",
   "id": "11834c89f3c91003",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.333225Z",
     "start_time": "2025-06-07T14:42:33.329376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "choosen_recommender_profile = [\n",
    "    Test(name=\"HybridRecommender\", type=\"hybrid\", mode=\"user\", k_value=5, second_k_value=14, metric=\"pearson\", calculation_variety=\"weighted\", alpha=0.5),\n",
    "    Test(name=\"HybridRecommender\", type=\"hybrid\", mode=\"user\", k_value=5, second_k_value=14, metric=\"cosine\", calculation_variety=\"weighted\", alpha=0.5)\n",
    "]"
   ],
   "id": "3b9bbc3845fde0ce",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T14:42:33.540535Z",
     "start_time": "2025-06-07T14:42:33.349744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tester = MAETester(\n",
    "        tests=choosen_recommender_profile,\n",
    "        test_data_path=testdata_path,\n",
    "        data_path=\"./data/Bewertungsmatrix_FlixNet.csv\",\n",
    "        user_ratings=\"./data/Ratings_FlixNet.csv\",\n",
    "        item_profile_path=\"./data/Itemprofile_FlixNet.csv\",\n",
    "    )"
   ],
   "id": "1a177fda3e3854fd",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/Testdaten_FlixNet.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[45]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m tester = \u001B[43mMAETester\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtests\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchoosen_recommender_profile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest_data_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtestdata_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./data/Bewertungsmatrix_FlixNet.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_ratings\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./data/Ratings_FlixNet.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m        \u001B[49m\u001B[43mitem_profile_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./data/Itemprofile_FlixNet.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mMAETester.__init__\u001B[39m\u001B[34m(self, tests, test_data_path, data_path, item_profile_path, user_ratings)\u001B[39m\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, tests: List[Test], test_data_path: \u001B[38;5;28mstr\u001B[39m, data_path: \u001B[38;5;28mstr\u001B[39m, item_profile_path: \u001B[38;5;28mstr\u001B[39m, user_ratings: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     33\u001B[39m     \u001B[38;5;28mself\u001B[39m.tests = tests\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     \u001B[38;5;28mself\u001B[39m.testdata = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data_path\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# testdata (for evaluaton)\u001B[39;00m\n\u001B[32m     35\u001B[39m     \u001B[38;5;28mself\u001B[39m.item_profile = pd.read_csv(item_profile_path)\n\u001B[32m     36\u001B[39m     \u001B[38;5;28mself\u001B[39m.user_ratings = pd.read_csv(user_ratings)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gruppenprojekt-hGfnXgKr-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gruppenprojekt-hGfnXgKr-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gruppenprojekt-hGfnXgKr-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gruppenprojekt-hGfnXgKr-py3.11\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\gruppenprojekt-hGfnXgKr-py3.11\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: './data/Testdaten_FlixNet.csv'"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "result = tester.run_tests()\n",
    "result # print result here"
   ],
   "id": "1259e7059201639c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
